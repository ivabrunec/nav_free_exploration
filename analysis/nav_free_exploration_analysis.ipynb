{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of free exploration trajectories in VR\n",
    "<br>\n",
    "This notebook contains clean code to preprocess, visualize, and analyze free roaming navigation data. The analyses included in this notebook are experienced integration, and roaming entropy, as well as mean squared displacement. <br>\n",
    "The set up assumes that there are 4 columns in the navigation logs (x, y, z, heading). The sampling rate in the present dataset is 10 Hz. <br>\n",
    "For now, we have only included 5 participants' navigation logs to help with the speed of processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Navigation log preprocessing](#preprocessing)\n",
    "2. [Trajectory visualization](#trajectories)\n",
    "3. [Experienced integration](#integration)\n",
    "4. [Roaming entropy](#entropy)\n",
    "5. [Mean squared displacement](#msd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation log preprocessing <a name=\"preprocessing\"></a>\n",
    "First, the navigation logs are cleaned and preprocessed for further analyses. Any periods where the participants were stationary for more than 30 seconds are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing ####\n",
    "path = r'.\\navlogs' # subfolder with nav logs\n",
    "all_files = glob.glob(path + \"\\\\0*.txt\")\n",
    "\n",
    "# initialize empty list\n",
    "data = []\n",
    "\n",
    "for csv in all_files:\n",
    "    # initialize counter to remove periods of > 30s\n",
    "    j = 0\n",
    "    frame = pd.read_csv(csv, sep=\";\", names=['x', 'y', 'z', 'heading'], header=None)\n",
    "    total_dur = len(frame)\n",
    "    print('currently preprocessing', os.path.basename(csv))\n",
    "    # flip coordinates to make the map upright\n",
    "    frame.x = -1*frame.x\n",
    "    frame.z = -1*frame.z\n",
    "    # store raw row indices\n",
    "    frame['row_num'] = frame.index\n",
    "    frame['chunk_index'] = \"\"\n",
    "    # calculate timepoint-to-timepoint displacement to identify stationary periods\n",
    "    r = np.sqrt(frame.x**2 + frame.z**2)\n",
    "    frame['diff'] = r.diff(-1) \n",
    "    # now loop over rows & append a label to each chunk\n",
    "    # iterrows is slow but it works\n",
    "    for index, row in frame.iterrows():\n",
    "        if (frame.loc[index,'diff'] == 0 and frame.loc[index+1, 'diff'] == 0):\n",
    "            frame.loc[index,'chunk_index'] = j\n",
    "        else: \n",
    "            j = j + 1\n",
    "            frame.loc[index,'chunk_index'] = \"mov\"\n",
    "    # add count for each data chunk\n",
    "    frame['count'] = frame.groupby('chunk_index')['chunk_index'].transform('count') \n",
    "    # keep only chunks where participants were moving, or where the stationary count is >= 30 seconds\n",
    "    frame = frame[(frame['chunk_index'] == \"mov\") | (frame['count'] < 300)]\n",
    "   \n",
    "    print('done with', os.path.basename(csv))\n",
    "    \n",
    "    # save clean nav log as csv for future use\n",
    "    filename = os.path.basename(csv)\n",
    "    sub_id = filename.split('_', 1)[0]\n",
    "    frame['sub'] = sub_id\n",
    "    path=r'.\\navlogs_clean\\\\'\n",
    "    \n",
    "    csv_filename = path + sub_id + '_clean_navlog.csv'\n",
    "    frame.to_csv(csv_filename)\n",
    "    \n",
    "    # store each dataset in the list for the next step\n",
    "    data.append(frame)\n",
    "\n",
    "print('*** done preprocessing nav logs ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory visualization <a name=\"trajectories\"></a>\n",
    "\n",
    "We can now display all of the participants' navigation trajectories in Virtual Silcton. To adapt this to another data set, you need to read in your image of the environment and transform the x & z coordinates to match the image. This transformation step is for <i> visualization only</i>. It does not affect any of the exploration measures, so it's not a problem if the image is slightly misaligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment if you need to read in again ##\n",
    "# path = r'.\\navlogs_clean'\n",
    "# all_files = glob.glob(path + \"\\\\00*.csv\")\n",
    "\n",
    "# data=[]\n",
    "# for csv in all_files:\n",
    "#    frame = pd.read_csv(csv, sep=\",\")\n",
    "#    data.append(frame)\n",
    "\n",
    "bigframe = pd.concat(data)\n",
    "bigframe = bigframe.dropna() \n",
    "x = pd.to_numeric(bigframe['x'])\n",
    "z = pd.to_numeric(bigframe['z'])\n",
    "\n",
    "# match image coordinates\n",
    "x= x + 738\n",
    "z= z + 508\n",
    "\n",
    "im = plt.imread(\"Full_Info_Silcton_Map.png\")\n",
    "implot = plt.imshow(im, extent = [0, 1450, 170, 1030])\n",
    "plt.scatter(x, z, c = bigframe['heading'], s = 8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experienced integration <a name=\"integration\"></a>\n",
    "\n",
    "Now that the navigation logs are preprocessed, we can look at some exploration measures. <br> First, we will relate participants' navigation tendencies to a measure of space syntax called <i>axial integration</i>. This measure expresses each segment in the environment in terms of its topological proximity to every other segment. Higher values therefore reflect higher integration with the rest of the environment. <br>\n",
    "In our analysis of exploration patterns, we will apply the corresponding integration value to each coordinate in space traversed by each participant. We will only keep coordinates that are < 10 virtual meters away from the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot integration map & overlay trajectories ##\n",
    "map_file = pd.read_csv(\"VGA_integration_populated.txt\", sep=\"\\t\")\n",
    "\n",
    "path = r'.\\navlogs_clean'\n",
    "all_files = glob.glob(path + \"\\\\*.csv\")\n",
    "\n",
    "data=[]\n",
    "for csv in all_files:\n",
    "    frame = pd.read_csv(csv, sep=\",\")\n",
    "    data.append(frame)\n",
    "\n",
    "bigframe = pd.concat(data)\n",
    "bigframe = bigframe.dropna() \n",
    "x = pd.to_numeric(bigframe['x'])\n",
    "z = pd.to_numeric(bigframe['z'])\n",
    "\n",
    "cmap = ListedColormap(sns.color_palette(\"viridis\", 20))   \n",
    "plt.scatter(map_file.x, map_file.y, c = map_file.integ, cmap=cmap, s = 5)\n",
    "plt.scatter(x, z, c = 'grey', s = 5, alpha = .1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the idea is to extract the corresponding integration value from the map & create a timeseries for each coordinate each participant traversed in the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in map file \n",
    "map_file = pd.read_csv(\"VGA_integration_populated.txt\", sep=\"\\t\")\n",
    "\n",
    "# save as array to match coordinates\n",
    "map_arr = map_file[['x', 'y']].to_numpy()\n",
    "\n",
    "path = r'.\\navlogs_clean'\n",
    "all_files = glob.glob(path + \"\\\\*.csv\")\n",
    "\n",
    "data=[]\n",
    "for csv in all_files:\n",
    "    frame = pd.read_csv(csv, sep=\",\")\n",
    "    data.append(frame)\n",
    "\n",
    "# initialize empty list\n",
    "data_integ = []\n",
    "\n",
    "for frame in data:\n",
    "    # append total nav duration\n",
    "    frame['total_dur'] = len(frame)\n",
    "    print('currently processing', frame['sub'].iloc[0])\n",
    "    # initialize columns with distance from integration map & integration values\n",
    "    frame['distance_coord'] = \"\"\n",
    "    frame['conn'] = \"\"\n",
    "    # go through rows, append the integration value & distance from map\n",
    "    # !! this step takes a while !! this is because it iterates over rows.\n",
    "    # there must be a smarter way to do this, but this was the only way that I ended up finding that worked\n",
    "    # spatial.kdtree works by finding the nearest point to each pair of x,z coordinates\n",
    "    for index, row in frame.iterrows():\n",
    "        distance,map_index = spatial.KDTree(map_arr).query([frame.x[index], frame.z[index]])\n",
    "        frame.loc[index, 'distance_coord'] = distance\n",
    "        frame.loc[index, 'integ'] = map_file['integ'][map_index]\n",
    "    # remove points where participants went off-path by more than 10 vm\n",
    "    frame = frame[frame['distance_coord'] < 10] \n",
    "    # append duration spent on path (> 10 vm off-path)\n",
    "    dur_on_path = len(frame)\n",
    "    frame['dur_on_path'] = dur_on_path\n",
    "    \n",
    "    data_integ.append(frame)\n",
    "    \n",
    "print('*** done with experienced integration analysis ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate into one dataframe\n",
    "# uncomment below to write to csv\n",
    "integ_frame = pd.concat(data_integ, ignore_index = False) \n",
    "#integ_frame.to_csv('silcton_freeexplore_integ_vals_clean.csv')\n",
    "\n",
    "# summarise\n",
    "# uncomment below to write to csv\n",
    "integ_frame['integ'] = integ_frame['integ'].astype(float)\n",
    "mean_integ = integ_frame.groupby(['sub']).agg({'integ': 'mean'})\n",
    "#mean_integ.to_csv('silcton_freeexplore_integ_per_sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, to make it more intuitive, we can plot the raw trajectories + the trajectories 'weighted' by their integration value. This also shows where the integration values got filtered out off-path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize interactive seaborn color picker\n",
    "my_palette = sns.choose_cubehelix_palette()\n",
    "# parameters to match published color palette:\n",
    "# n_colors = 15\n",
    "# start = 1.10\n",
    "# rot = -1.00\n",
    "# gamma = 0.70\n",
    "# hue = 1.00\n",
    "# light = 0.70\n",
    "# dark = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to cmap format\n",
    "cmap = ListedColormap(my_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize overlap between integration and trajectories ##\n",
    "\n",
    "# read in overall frame with all data\n",
    "connframe = pd.read_csv(\"silcton_freeexplore_integ_vals_clean.csv\", sep=\",\")\n",
    "\n",
    "# match image coordinates\n",
    "connframe.x = connframe.x + 738\n",
    "connframe.z = connframe.z + 510\n",
    "\n",
    "path = r'.\\navlogs_clean'\n",
    "all_files = glob.glob(path + \"\\\\*.csv\")\n",
    "\n",
    "data=[]\n",
    "for csv in all_files:\n",
    "    frame = pd.read_csv(csv, sep=\",\")\n",
    "    data.append(frame)\n",
    "\n",
    "rawframe = pd.concat(data)\n",
    "rawframe = rawframe.dropna() \n",
    "x = pd.to_numeric(rawframe['x'])\n",
    "z = pd.to_numeric(rawframe['z'])\n",
    "\n",
    "# match image coordinates\n",
    "x = x + 738\n",
    "z = z + 510\n",
    "\n",
    "im = plt.imread(\"Full_Info_Silcton_Map.png\")\n",
    "implot = plt.imshow(im, extent = [0, 1456, 170, 1032], alpha = .5)\n",
    "\n",
    "plt.scatter(x, z, c = \"grey\", alpha = .05, s = .5)\n",
    "plt.scatter(connframe.x, connframe.z, c = connframe['integ'], cmap = cmap, alpha = 1, s = 1) \n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# uncomment below to save figure\n",
    "#plt.savefig('integration_x_traces.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roaming entropy <a name=\"entropy\"></a>\n",
    "As a second exploration measure, let's calculate each participant's roaming entropy. This translates to the coverage of the environment. Participants with low roaming entropy cover fewer states relative to those with high roaming entropy. In the present analysis, we divided the environment into 50 x 50 states. This can easily be adjusted by changing the number in the 'pd_cut' command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Roaming Entropy ###\n",
    "## read in clean datasets\n",
    "path = r'.\\navlogs_clean'\n",
    "all_files = glob.glob(path + \"\\\\*.csv\")\n",
    "\n",
    "data=[]\n",
    "for csv in all_files:\n",
    "    frame = pd.read_csv(csv, sep=\",\")\n",
    "    data.append(frame)\n",
    "\n",
    "# initialize empty frames\n",
    "data_states = []\n",
    "\n",
    "# in this loop, we'll first count up the number of timepoints each participant spent in each state\n",
    "for frame in data:\n",
    "    print('calculating state exploration for', frame['sub'].iloc[0])\n",
    "    # split into 50 x 50 grid\n",
    "    frame['x_cut_50'] = pd.cut(frame.x, np.linspace(-738, 738, 51), right=False)\n",
    "    frame['y_cut_50'] = pd.cut(frame.z, np.linspace(-523, 523, 51), right=False)\n",
    "    # example: division into 20 x 20 grid\n",
    "    #frame['x_cut_20'] = pd.cut(frame.x, np.linspace(-738, 738, 21), right=False)\n",
    "    #frame['y_cut_20'] = pd.cut(frame.z, np.linspace(-523, 523, 21), right=False)\n",
    "    frame['count_events'] = \"\"\n",
    "    count_frame = frame.groupby(['x_cut_50', 'y_cut_50']).agg({'count_events': ['count']})\n",
    "    count_frame['sub'] = frame['sub'].iloc[0]\n",
    "    data_states.append(count_frame)\n",
    "\n",
    "print('*** done with state exploration ***')\n",
    "states_frame = pd.concat(data_states, ignore_index=False) \n",
    "states_frame.reset_index(inplace=True)\n",
    "states_frame.columns = ['x_cut', 'y_cut', 'count', 'sub']\n",
    "# uncomment to write to csv\n",
    "#states_frame.to_csv('silcton_freeexplore_50_states_count.csv')\n",
    "\n",
    "# now, we'll calculate roaming entropy for each participant\n",
    "cur_dim = states_frame.groupby(['sub']).size()\n",
    "cur_dim = cur_dim.iloc[0]\n",
    "\n",
    "def roaming_entropy(event_count):\n",
    "    event_count = event_count[event_count != 0]\n",
    "    norm_counts = event_count/np.sum(event_count)    \n",
    "    return -(norm_counts * np.log2(norm_counts)/np.log(cur_dim)).sum()\n",
    "\n",
    "entropy_frame = states_frame.groupby('sub').apply(lambda x: roaming_entropy(x['count']))\n",
    "\n",
    "# uncomment to write to csv\n",
    "#entropy_frame.to_csv(\"silcton_roaming_entropy_per_sub.csv\")\n",
    "print('*** done with entropy calculation ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared displacement <a name=\"msd\"></a>\n",
    "\n",
    "The final measure we included in our overall model is Mean Squared Displacement. It essentially translates to <i>speed</i> of movement in the environment. However, it does not necessarily relate to coverage; a participant can move a lot in a small area of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty lists\n",
    "data_msd = []\n",
    "data_diff_sq = []\n",
    "\n",
    "path = r'.\\navlogs_clean'\n",
    "all_files = glob.glob(path + \"\\\\*clean_navlog.csv\")\n",
    "\n",
    "data=[]\n",
    "for csv in all_files:\n",
    "    frame = pd.read_csv(csv, sep=\",\")\n",
    "    data.append(frame)\n",
    "\n",
    "for frame in data:\n",
    "    print('calculating MSD for', frame['sub'].iloc[0])\n",
    "    # we have already calculated the 'diff' column earlier to detect periods of movement\n",
    "    # now, all we need to do is to square that value & average across entire timecourse to calculate mean square displacement\n",
    "    diff_sq = frame['diff']**2\n",
    "    MSD = np.mean(diff_sq)\n",
    "    data_msd.append(MSD)\n",
    "    \n",
    "    ## to calculate MSD at longer lags:\n",
    "    ## uncomment below and change the value in the 'r.diff' operation, e.g. -2, -3 ...\n",
    "    #r = np.sqrt(frame.x**2 + frame.z**2)\n",
    "    #frame['diff'] = r.diff(-2) \n",
    "    #diff_sq = frame['diff']**2\n",
    "    #MSD = np.mean(diff_sq)\n",
    "    #data_msd.append(MSD)\n",
    "    \n",
    "    # create a new dataframe with moment-by-moment values for later use\n",
    "    diff_dset = pd.DataFrame(diff_sq)\n",
    "    sub_id = frame['sub'].iloc[0]\n",
    "    diff_dset['sub'] = sub_id\n",
    "    diff_dset['row_num'] = frame['row_num']\n",
    "    data_diff_sq.append(diff_dset)\n",
    "\n",
    "print('*** done with MSD calculation ***')\n",
    "\n",
    "# uncomment below to save dataframes to csv\n",
    "diff_sq_frame = pd.concat(data_diff_sq, ignore_index=False) \n",
    "#diff_sq_frame.to_csv('silcton_diff_sq_timepoints_lag1.csv')\n",
    "data_msd = pd.DataFrame(data_msd)\n",
    "#data_msd.to_csv(\"silcton_msd_lag1_per_sub.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap of all trajectories\n",
    "## 50 x 50 grid as in the entropy calculations\n",
    "\n",
    "path = r'.\\navlogs_clean'\n",
    "all_files = glob.glob(path + \"\\\\0*.csv\")\n",
    "\n",
    "data=[]\n",
    "for csv in all_files:\n",
    "    frame = pd.read_csv(csv, sep=\",\")\n",
    "    data.append(frame)\n",
    "\n",
    "bigframe = pd.concat(data)\n",
    "bigframe = bigframe.dropna() \n",
    "\n",
    "x = pd.to_numeric(bigframe['x'])\n",
    "z = pd.to_numeric(bigframe['z'])\n",
    "\n",
    "# adjust coordinates to overlap with the map image\n",
    "x = x + 750\n",
    "z = z + 510\n",
    "\n",
    "im = plt.imread(\"Full_Info_Silcton_Map.png\")\n",
    "plt.imshow(im, extent = [0, 1460, 160, 1038], alpha = .5, aspect = 'auto')\n",
    "\n",
    "plt.hist2d(x, z, bins=(50, 50), cmin = .1, range = [[0, 1500], [0, 1038]], cmap = \"Spectral_r\", alpha = .85) \n",
    "plt.colorbar()\n",
    "plt.clim(0,600)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# uncomment to save figure\n",
    "#plt.savefig('heatmap_50x50.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot locations where participants stopped and visually scanned their environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in map file \n",
    "map_file = pd.read_csv(\"VGA_integration_populated.txt\", sep=\"\\t\")\n",
    "\n",
    "map_file.x = map_file.x + 738\n",
    "map_file.y = map_file.y + 510\n",
    "\n",
    "# save as array to match coordinates\n",
    "map_arr = map_file[['x', 'y']].to_numpy()\n",
    "\n",
    "frame = pd.read_csv('silcton_detected_scanning_events_clean.csv', sep=\",\")\n",
    "\n",
    "plt.scatter(frame.x_pos, frame.z_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
